<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.31">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Yash Mali">

<title>AttenDence: Maximizing Attention Confidence for Test Time Adaptation</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="attendence_files/libs/clipboard/clipboard.min.js"></script>
<script src="attendence_files/libs/quarto-html/quarto.js" type="module"></script>
<script src="attendence_files/libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="attendence_files/libs/quarto-html/popper.min.js"></script>
<script src="attendence_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="attendence_files/libs/quarto-html/anchor.min.js"></script>
<link href="attendence_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="attendence_files/libs/quarto-html/quarto-syntax-highlighting-e1a5c8363afafaef2c763b6775fbf3ca.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="attendence_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="attendence_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="attendence_files/libs/bootstrap/bootstrap-837b9f7832c7cdb21bb0ea321b3f88e2.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<style>html{ scroll-behavior: smooth; }</style>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="quarto-light">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-full">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#abstract" id="toc-abstract" class="nav-link active" data-scroll-target="#abstract"><span class="header-section-number">1</span> Abstract</a></li>
  <li><a href="#introduction" id="toc-introduction" class="nav-link" data-scroll-target="#introduction"><span class="header-section-number">2</span> 1 Introduction</a></li>
  <li><a href="#method" id="toc-method" class="nav-link" data-scroll-target="#method"><span class="header-section-number">3</span> 2 Method</a>
  <ul class="collapse">
  <li><a href="#problem-setting" id="toc-problem-setting" class="nav-link" data-scroll-target="#problem-setting"><span class="header-section-number">3.1</span> 2.1 Problem Setting</a></li>
  <li><a href="#tta-objective-attention-entropy-minimization" id="toc-tta-objective-attention-entropy-minimization" class="nav-link" data-scroll-target="#tta-objective-attention-entropy-minimization"><span class="header-section-number">3.2</span> 2.2 TTA Objective: Attention Entropy Minimization</a></li>
  <li><a href="#optimization-and-adaptation-protocol" id="toc-optimization-and-adaptation-protocol" class="nav-link" data-scroll-target="#optimization-and-adaptation-protocol"><span class="header-section-number">3.3</span> 2.3 Optimization and Adaptation Protocol</a></li>
  </ul></li>
  <li><a href="#figures" id="toc-figures" class="nav-link" data-scroll-target="#figures"><span class="header-section-number">4</span> 3 Figures</a>
  <ul class="collapse">
  <li><a href="#attention-overview" id="toc-attention-overview" class="nav-link" data-scroll-target="#attention-overview"><span class="header-section-number">4.1</span> 3.1 Attention Overview</a></li>
  <li><a href="#optimization-experiments" id="toc-optimization-experiments" class="nav-link" data-scroll-target="#optimization-experiments"><span class="header-section-number">4.2</span> 3.2 Optimization Experiments</a></li>
  </ul></li>
  <li><a href="#related-work" id="toc-related-work" class="nav-link" data-scroll-target="#related-work"><span class="header-section-number">5</span> 4 Related Work</a>
  <ul class="collapse">
  <li><a href="#test-time-adaptation" id="toc-test-time-adaptation" class="nav-link" data-scroll-target="#test-time-adaptation"><span class="header-section-number">5.1</span> 4.1 Test-Time Adaptation</a></li>
  </ul></li>
  <li><a href="#experiments" id="toc-experiments" class="nav-link" data-scroll-target="#experiments"><span class="header-section-number">6</span> 5 Experiments</a>
  <ul class="collapse">
  <li><a href="#experimental-setup" id="toc-experimental-setup" class="nav-link" data-scroll-target="#experimental-setup"><span class="header-section-number">6.1</span> 5.1 Experimental Setup</a></li>
  <li><a href="#main-results" id="toc-main-results" class="nav-link" data-scroll-target="#main-results"><span class="header-section-number">6.2</span> 5.2 Main Results</a></li>
  </ul></li>
  <li><a href="#limitations" id="toc-limitations" class="nav-link" data-scroll-target="#limitations"><span class="header-section-number">7</span> 6 Limitations</a>
  <ul class="collapse">
  <li><a href="#architecture-dependence" id="toc-architecture-dependence" class="nav-link" data-scroll-target="#architecture-dependence"><span class="header-section-number">7.1</span> 6.1 Architecture Dependence</a></li>
  <li><a href="#attention-quality-dependency" id="toc-attention-quality-dependency" class="nav-link" data-scroll-target="#attention-quality-dependency"><span class="header-section-number">7.2</span> 6.2 Attention Quality Dependency</a></li>
  <li><a href="#computational-overhead" id="toc-computational-overhead" class="nav-link" data-scroll-target="#computational-overhead"><span class="header-section-number">7.3</span> 6.3 Computational Overhead</a></li>
  <li><a href="#potential-for-overconfidence" id="toc-potential-for-overconfidence" class="nav-link" data-scroll-target="#potential-for-overconfidence"><span class="header-section-number">7.4</span> 6.4 Potential for Overconfidence</a></li>
  </ul></li>
  <li><a href="#future-research-directions" id="toc-future-research-directions" class="nav-link" data-scroll-target="#future-research-directions"><span class="header-section-number">8</span> 7 Future Research Directions</a>
  <ul class="collapse">
  <li><a href="#multi-level-attention" id="toc-multi-level-attention" class="nav-link" data-scroll-target="#multi-level-attention"><span class="header-section-number">8.1</span> 7.1 Multi-Level Attention</a></li>
  <li><a href="#optimizer-memory" id="toc-optimizer-memory" class="nav-link" data-scroll-target="#optimizer-memory"><span class="header-section-number">8.2</span> 7.2 Optimizer Memory</a></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"><span class="header-section-number">9</span> 8 Conclusion</a></li>
  <li><a href="#acknowledgements" id="toc-acknowledgements" class="nav-link" data-scroll-target="#acknowledgements"><span class="header-section-number">10</span> Acknowledgements</a></li>
  </ul>
</nav>
</div>
<main class="content column-page-left" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">AttenDence: Maximizing Attention Confidence for Test Time Adaptation</h1>
</div>


<div class="quarto-title-meta-author column-page-left">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author">Yash Mali <a href="mailto:ymali@mail.ubc.ca" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            University of British Columbia
          </p>
      </div>
  </div>

<div class="quarto-title-meta column-page-left">

      
  
    
  </div>
  


</header>


<style>
/* Mobile-first responsive design (from your CV theme, simplified) */
@media (max-width: 768px) {
  .columns {
    display: block !important;
  }

  .column {
    width: 100% !important;
    margin-bottom: 2rem;
  }

  .photo-circle {
    width: 120px !important;
    max-width: 120px !important;
  }

  body {
    font-size: 0.9em !important;
    line-height: 1.4;
  }

  h1 {
    font-size: 1.8rem;
  }

  h2 {
    font-size: 1.4rem;
  }

  h3 {
    font-size: 1.2rem;
  }

  .quarto-container {
    padding: 1rem;
  }

  a {
    padding: 0.2rem;
    margin: 0.1rem;
  }

  p, li {
    margin-bottom: 0.8rem;
  }
}

/* Tablet adjustments */
@media (max-width: 1024px) and (min-width: 769px) {
  body {
    font-size: 0.8em !important;
  }
}

/* General improvements */
img {
  max-width: 100%;
  height: auto;
}

.photo-circle {
  border-radius: 50%;
  object-fit: cover;
  height: auto;
}

ul {
  padding-left: 1.2rem;
}

li {
  margin-bottom: 1rem;
}
</style>
<p><strong>Author:</strong> <em>Yash Mali</em><br>
<strong>Affiliation:</strong> University of British Columbia<br>
<strong>Contact:</strong> <a href="mailto:ymali@mail.ubc.ca">ymali@mail.ubc.ca</a></p>
<hr>
<section id="abstract" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="abstract"><span class="header-section-number">1</span> Abstract</h2>
<p>Test-time adaptation (TTA) enables models to adapt to distribution shifts at inference time. While entropy minimization over the output distribution has proven effective for TTA, transformers offer an additional unsupervised learning signal through their attention mechanisms. We propose minimizing the entropy of attention distributions from the CLS token to image patches as a novel TTA objective. This approach encourages the model to attend more confidently to relevant image regions under distribution shift and is effective even when only a single test image is available. We demonstrate that attention entropy minimization substantially improves robustness across diverse corruption types while not hurting performance on clean data.</p>
<hr>
</section>
<section id="introduction" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="introduction"><span class="header-section-number">2</span> 1 Introduction</h2>
<p>Deep neural networks achieve impressive performance on in-distribution data but often fail catastrophically when deployed on data from shifted distributions <span class="citation" data-cites="hendrycks2019robustness">[@hendrycks2019robustness]</span>. This brittleness poses significant challenges for real-world deployment where models encounter diverse corruptions and covariate shifts. TTA addresses this challenge by adapting models to new distributions.</p>
<p>Recent TTA methods have primarily focused on entropy minimization over the output distribution, which encourages the model to make confident predictions at test time. While effective, this approach treats the feature extractor as a black box and ignores rich internal representations that could guide adaptation. Vision transformers <span class="citation" data-cites="dosovitskiy2021an">[@dosovitskiy2021an]</span>, which have become the dominant architecture for visual recognition, offer a particularly promising internal signal: attention mechanisms that explicitly capture spatial relationships and feature importance.</p>
<p>In this work, we introduce a novel TTA objective based on minimizing the entropy of attention distributions in vision transformers. Specifically, we minimize the entropy of the distribution defined by the attention scores of the CLS token attending to the image patch tokens. This encourages the model to focus more confidently on relevant image regions under distribution shift. Our approach is motivated by two key observations:</p>
<p>First, entropy minimization on output predictions works because confident predictions tend to be correct predictions. We hypothesize that this principle extends to attention mechanisms: confident attention patterns (low entropy) should correspond to better feature representations.</p>
<p>Second, modern vision transformers like DINOv3 <span class="citation" data-cites="simeoni2025dinov3">[@simeoni2025dinov3]</span> learn rich, interpretable attention maps through internet-scale self-supervised training.</p>
<hr>
</section>
<section id="method" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="method"><span class="header-section-number">3</span> 2 Method</h2>
<section id="problem-setting" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="problem-setting"><span class="header-section-number">3.1</span> 2.1 Problem Setting</h3>
<p>We consider the standard test-time adaptation setting. Given a model (f_) trained on a source distribution (<em>{}), we encounter a sample (x_i) from a shifted distribution (</em>{}) at test time.</p>
<p>For each test sample (x_i _{}), we aim to:</p>
<ol type="1">
<li>Adapt the model parameters () using an unsupervised objective ((x_i; )).</li>
<li>Generate prediction (<em>i = f</em>(x_i)) using the adapted model.</li>
<li>Reset the model parameters to the original pretrained state.</li>
</ol>
<p>We focus on the fully test-time setting where no source data, validation data, or labels are available during adaptation. This is the most challenging and practical setting for TTA.</p>
</section>
<section id="tta-objective-attention-entropy-minimization" class="level3" data-number="3.2">
<h3 data-number="3.2" class="anchored" data-anchor-id="tta-objective-attention-entropy-minimization"><span class="header-section-number">3.2</span> 2.2 TTA Objective: Attention Entropy Minimization</h3>
<p>Our method makes use of the attention mechanism in vision transformers to guide test-time adaptation. Vision transformers process images as sequences of patches, with a CLS token that aggregates global information for downstream tasks like classification. The self-attention mechanism computes pairwise relationships between all tokens, producing attention weights that indicate the importance of each token.</p>
<p>We focus on the attention from the CLS token to image patch tokens in the final transformer layer. These attention weights can be interpreted as indicating which image regions are most relevant for the classification decision. Under distribution shift, we hypothesize that attention patterns may become more diffuse or uncertain, indicating that the model is unsure where to focus.</p>
<section id="mathematical-formulation" class="level4" data-number="3.2.1">
<h4 data-number="3.2.1" class="anchored" data-anchor-id="mathematical-formulation"><span class="header-section-number">3.2.1</span> 2.2.1 Mathematical Formulation</h4>
<p>Let the final-layer attention tensor be</p>
<p><span class="math display">\[
\mathbf{A} \in \mathbb{R}^{B \times H \times T \times T},
\]</span></p>
<p>where:</p>
<ul>
<li>(B) is the batch size (here (B = 1)),</li>
<li>(H) is the number of attention heads,</li>
<li>(T) is the sequence length (CLS token + register tokens + patch tokens).</li>
</ul>
<p>Let (t_{}) denote the index of the CLS token, and let (t_r) be the number of register or special tokens (including the CLS token itself). Patch tokens correspond to indices</p>
<p><span class="math display">\[
j = t_r + 1, \dots, T.
\]</span></p>
<p>We extract the CLS-to-patch attention scores as</p>
<p><span class="math display">\[
A_{\text{cls}}(h, j)
=
\mathbf{A}_{1,\,h,\,t_{\text{cls}},\,j},
\qquad
h = 1,\dots,H,\;\;
j = t_r + 1,\dots,T,
\]</span></p>
<p>giving a matrix</p>
<p><span class="math display">\[
A_{\text{cls}} \in \mathbb{R}^{H \times (T - t_r)}.
\]</span></p>
<p>We then average across attention heads:</p>
<p><span class="math display">\[
\bar{a}_j
=
\frac{1}{H}
\sum_{h=1}^{H}
A_{\text{cls}}(h, j),
\qquad
j = t_r + 1,\dots,T.
\]</span></p>
<p>Next, we normalize to form a probability distribution over patch tokens (since we exclude register tokens):</p>
<p><span class="math display">\[
\tilde{a}_j
=
\frac{\bar{a}_j}{\sum_{k = t_r + 1}^{T} \bar{a}_k},
\qquad
j = t_r + 1,\dots,T.
\]</span></p>
<p>The attention entropy loss is then defined as:</p>
<p><span class="math display">\[
\mathcal{L}_{\text{attn}}
=
- \sum_{j = t_r + 1}^{T}
\tilde{a}_j \log(\tilde{a}_j).
\]</span></p>
<p>Minimizing this loss encourages the attention from the CLS token to be focused (i.e., low-entropy) over patch tokens.</p>
</section>
<section id="intuition-and-design-choices" class="level4" data-number="3.2.2">
<h4 data-number="3.2.2" class="anchored" data-anchor-id="intuition-and-design-choices"><span class="header-section-number">3.2.2</span> 2.2.2 Intuition and Design Choices</h4>
<p>Several design choices are worth noting:</p>
<p><strong>CLS token attention.</strong> We focus on attention from the CLS token because it directly influences the features used for classification. Other tokens (patch tokens or register tokens) have different semantic roles and may not be as informative for adaptation.</p>
<p><strong>Final layer.</strong> We use the final transformer layer because it contains the highest-level semantic information. Earlier layers may capture low-level features that are less relevant for classification.</p>
<p><strong>Excluding register tokens.</strong> DINOv3 uses register tokens that serve as additional global memory. These are not part of the image content and should be excluded from our attention distribution.</p>
<p><strong>L1 vs softmax normalization.</strong> We use L1 normalization rather than softmax because attention weights are already normalized within the transformer. Re-applying softmax would unnecessarily rescale the distribution.</p>
</section>
</section>
<section id="optimization-and-adaptation-protocol" class="level3" data-number="3.3">
<h3 data-number="3.3" class="anchored" data-anchor-id="optimization-and-adaptation-protocol"><span class="header-section-number">3.3</span> 2.3 Optimization and Adaptation Protocol</h3>
<p>For each test sample, we perform a single gradient descent step on the attention entropy loss and then reset the model parameters.</p>
<p>We explore Adam and SGD to perform the single update. We also experiment with whether to reset the optimizer state (e.g., first and second moments in Adam) between samples or allow it to accumulate over multiple test samples, while still resetting model parameters. We observe that not resetting Adam’s optimizer state between samples can sometimes improve performance, even though we reset the model parameters themselves.</p>
<hr>
</section>
</section>
<section id="figures" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="figures"><span class="header-section-number">4</span> 3 Figures</h2>
<section id="attention-overview" class="level3" data-number="4.1">
<h3 data-number="4.1" class="anchored" data-anchor-id="attention-overview"><span class="header-section-number">4.1</span> 3.1 Attention Overview</h3>
<p><img src="images/summ1.png" class="img-fluid"></p>
<p>The image patches are fed into the model, and the attention scores from the last attention block are taken. The CLS-to-image-patch scores define the distribution whose entropy is minimized.</p>
</section>
<section id="optimization-experiments" class="level3" data-number="4.2">
<h3 data-number="4.2" class="anchored" data-anchor-id="optimization-experiments"><span class="header-section-number">4.2</span> 3.2 Optimization Experiments</h3>
<div class="columns">
<div class="column" style="width:50%;">
<p><img src="images/adam_test_moments.png" class="img-fluid"></p>
<p><strong>Optimization with Adam and different moment-reset strategies.</strong></p>
</div><div class="column" style="width:50%;">
<p><img src="images/sgd_test_reset.png" class="img-fluid"></p>
<p><strong>Optimization with SGD and full parameter resets.</strong></p>
</div>
</div>
<hr>
</section>
</section>
<section id="related-work" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="related-work"><span class="header-section-number">5</span> 4 Related Work</h2>
<section id="test-time-adaptation" class="level3" data-number="5.1">
<h3 data-number="5.1" class="anchored" data-anchor-id="test-time-adaptation"><span class="header-section-number">5.1</span> 4.1 Test-Time Adaptation</h3>
<p>Test-time adaptation has emerged as a practical approach to handling distribution shift without requiring labeled target data or source data access.</p>
<p><strong>Tent.</strong> Tent <span class="citation" data-cites="wang2021tent">[@wang2021tent]</span> introduced entropy minimization for TTA, updating only batch normalization statistics to adapt to new distributions. This approach balances adaptation capability with stability by limiting the number of adapted parameters.</p>
<p><strong>TTT.</strong> TTT <span class="citation" data-cites="sun19ttt">[@sun19ttt]</span> proposes learning a self-supervised auxiliary task (like rotation) during training that can also be used for test-time optimization.</p>
<p><strong>MEMO.</strong> MEMO <span class="citation" data-cites="memo">[@memo]</span> uses test-time augmentation to turn a single image into a batch and minimizes the output entropy distribution.</p>
<hr>
</section>
</section>
<section id="experiments" class="level2" data-number="6">
<h2 data-number="6" class="anchored" data-anchor-id="experiments"><span class="header-section-number">6</span> 5 Experiments</h2>
<section id="experimental-setup" class="level3" data-number="6.1">
<h3 data-number="6.1" class="anchored" data-anchor-id="experimental-setup"><span class="header-section-number">6.1</span> 5.1 Experimental Setup</h3>
<p><strong>Dataset.</strong> We evaluate on CIFAR-10-C <span class="citation" data-cites="hendrycks2019robustness">[@hendrycks2019robustness]</span>, which augments the standard CIFAR-10 test set with 19 different corruption types at 5 severity levels. The corruptions span diverse categories.</p>
<p>We evaluate on severity level 5 (the most severe) for all corruption types. This provides a challenging test of robustness, as the corruptions are strong enough to significantly degrade performance.</p>
<p><strong>Model.</strong> We use DINOv3-base pretrained on a large-scale image dataset. We add a classification head and fine-tune the entire model on clean CIFAR-10 training data using standard cross-entropy loss.</p>
<p><strong>Evaluation Protocol.</strong> For each corruption type, we use 1,000 samples for hyperparameter search and then evaluate on 9,000 test samples. We report per-corruption accuracy and mean corruption accuracy (mCA) across all corruption types. Each sample (batch size 1) is adapted independently with a single gradient step.</p>
<p><strong>Baselines.</strong> We evaluate without any test-time updates, providing a measure of the model’s inherent robustness, and perform TTA on clean data to check if it hurts accuracy.</p>
</section>
<section id="main-results" class="level3" data-number="6.2">
<h3 data-number="6.2" class="anchored" data-anchor-id="main-results"><span class="header-section-number">6.2</span> 5.2 Main Results</h3>
<p>We can see from the figures that using attention confidence can be a useful self-supervised learning signal. We performed a hyperparameter sweep (e.g., learning rate, optimizer choice, number of adapted layers) and observe consistent improvements in robustness on corrupted data, without degrading performance on clean data.</p>
<!--
You can add a results table here, e.g.:

Method | mCA (%)
------ | -------
No Adaptation | XX.X
Tent (BN only) | XX.X
Output Entropy Min | XX.X
Attention Entropy Min (Ours) | **XX.X**
-->
<hr>
</section>
</section>
<section id="limitations" class="level2" data-number="7">
<h2 data-number="7" class="anchored" data-anchor-id="limitations"><span class="header-section-number">7</span> 6 Limitations</h2>
<p>While our method shows consistent improvements, we identify several limitations:</p>
<section id="architecture-dependence" class="level3" data-number="7.1">
<h3 data-number="7.1" class="anchored" data-anchor-id="architecture-dependence"><span class="header-section-number">7.1</span> 6.1 Architecture Dependence</h3>
<p>Our approach is specifically designed for vision transformers with attention mechanisms. CNNs and other architectures without explicit attention would require different adaptation strategies. This limits the generality of our method compared to architecture-agnostic approaches like output entropy minimization.</p>
</section>
<section id="attention-quality-dependency" class="level3" data-number="7.2">
<h3 data-number="7.2" class="anchored" data-anchor-id="attention-quality-dependency"><span class="header-section-number">7.2</span> 6.2 Attention Quality Dependency</h3>
<p>The effectiveness of attention entropy minimization depends on the quality of pretrained attention maps. We chose DINOv3 specifically because its self-supervised pretraining produces rich, interpretable attention patterns. Transformers with less interpretable attention (e.g., those trained purely on supervised ImageNet classification without registers) may benefit less from attention-based adaptation.</p>
</section>
<section id="computational-overhead" class="level3" data-number="7.3">
<h3 data-number="7.3" class="anchored" data-anchor-id="computational-overhead"><span class="header-section-number">7.3</span> 6.3 Computational Overhead</h3>
<p>We have to use a forward pass to capture attention scores, a backward pass to update the weights, and then another forward pass. This adds a significant computational overhead that may be warranted if TTA is needed.</p>
</section>
<section id="potential-for-overconfidence" class="level3" data-number="7.4">
<h3 data-number="7.4" class="anchored" data-anchor-id="potential-for-overconfidence"><span class="header-section-number">7.4</span> 6.4 Potential for Overconfidence</h3>
<p>Like output entropy minimization, attention entropy minimization encourages confident model behavior. On samples where the model is fundamentally uncertain (e.g., due to severe corruption or out-of-distribution inputs), forcing confident attention may lead to overconfident wrong predictions. Methods for detecting and handling high-uncertainty samples could improve robustness.</p>
<hr>
</section>
</section>
<section id="future-research-directions" class="level2" data-number="8">
<h2 data-number="8" class="anchored" data-anchor-id="future-research-directions"><span class="header-section-number">8</span> 7 Future Research Directions</h2>
<p>Our work opens several promising directions for future research:</p>
<section id="multi-level-attention" class="level3" data-number="8.1">
<h3 data-number="8.1" class="anchored" data-anchor-id="multi-level-attention"><span class="header-section-number">8.1</span> 7.1 Multi-Level Attention</h3>
<p>We focused on CLS-to-patch attention in the final layer. Future work could explore hierarchical attention patterns across multiple layers, potentially capturing both low-level and high-level adaptation signals. Different layers may be optimal for different types of distribution shift.</p>
</section>
<section id="optimizer-memory" class="level3" data-number="8.2">
<h3 data-number="8.2" class="anchored" data-anchor-id="optimizer-memory"><span class="header-section-number">8.2</span> 7.2 Optimizer Memory</h3>
<p>As an additional finding, we observed interesting interactions between optimizer state management and adaptation performance. Allowing moments to accumulate across samples (while still resetting model parameters) can improve performance, suggesting that temporal alignment via optimizer state may be an underexplored factor in TTA algorithm design.</p>
<hr>
</section>
</section>
<section id="conclusion" class="level2" data-number="9">
<h2 data-number="9" class="anchored" data-anchor-id="conclusion"><span class="header-section-number">9</span> 8 Conclusion</h2>
<p>We introduced attention entropy minimization as a novel objective for test-time adaptation with vision transformers. By encouraging confident attention patterns from CLS tokens to image patches, our method helps models focus on semantic content despite distribution shifts, improving robustness across diverse corruption types.</p>
<p>Our work demonstrates that vision transformers’ attention mechanisms provide valuable signals for test-time adaptation, complementing existing approaches based on output predictions. As transformers become increasingly prevalent, attention-based adaptation offers a promising direction for building more robust vision systems that can handle the distribution shifts encountered in real-world deployment.</p>
<hr>
</section>
<section id="acknowledgements" class="level2" data-number="10">
<h2 data-number="10" class="anchored" data-anchor-id="acknowledgements"><span class="header-section-number">10</span> Acknowledgements</h2>
<p>We thank Evan Shelhamer for helpful discussions.</p>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




<script src="attendence_files/libs/quarto-html/zenscroll-min.js"></script>
</body></html>